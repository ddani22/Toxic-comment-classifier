import streamlit as st
import joblib
import pandas as pd
from datetime import datetime

# Variables globales para modelos
clf = None
vectorizer = None
detailed_clf = None
detailed_vectorizer = None
toxicity_categories = None
load_error = None

# Cargar modelo b√°sico
try:
    clf = joblib.load("artifacts/model.pkl")
    vectorizer = joblib.load("artifacts/vectorizer.pkl")
    st.success("‚úÖ Modelo b√°sico cargado correctamente")
except Exception as e:
    load_error = e
    st.error(f"‚ùå Error cargando modelo b√°sico: {e}")

# Cargar modelo detallado (opcional)
detailed_available = False
try:
    detailed_clf = joblib.load("artifacts/detailed_model.pkl")
    detailed_vectorizer = joblib.load("artifacts/vectorizer_detailed.pkl")
    toxicity_categories = joblib.load("artifacts/toxicity_categories.pkl")
    detailed_available = True
    st.success("‚úÖ Modelo de clasificaci√≥n detallada disponible")
except Exception as e:
    st.warning(f"‚ö†Ô∏è Clasificaci√≥n detallada no disponible: {e}")

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Toxic Comment Classifier",
    page_icon="üîé",
    layout="wide"
)

st.title("üîé Clasificador de Comentarios T√≥xicos")
st.markdown("---")

# Sidebar con informaci√≥n del modelo
with st.sidebar:
    st.header("üìä Estado del Sistema")
    st.write(f"**Modelo b√°sico:** {'‚úÖ Activo' if clf is not None else '‚ùå Error'}")
    st.write(f"**Clasificaci√≥n detallada:** {'‚úÖ Disponible' if detailed_available else '‚ùå No disponible'}")
    
    if detailed_available:
        if toxicity_categories is not None:
            st.write(f"**Categor√≠as detectables:** {len(toxicity_categories)}")
        if detailed_vectorizer is not None and hasattr(detailed_vectorizer, 'vocabulary_'):
            st.write(f"**Vocabulario:** {len(detailed_vectorizer.vocabulary_):,} palabras")
        else:
            st.write("**Vocabulario:** No disponible")

# Input del usuario
col1, col2 = st.columns([2, 1])

with col1:
    user_input = st.text_area(
        "Escribe un comentario para analizar:",
        height=100,
        placeholder="Ejemplo: Este es un comentario de prueba..."
    )

with col2:
    st.markdown("### Opciones de an√°lisis")
    show_probability = st.checkbox("Mostrar probabilidades", value=True)
    show_detailed = st.checkbox(
        "An√°lisis detallado", 
        value=detailed_available,
        disabled=not detailed_available
    )

# Bot√≥n de clasificaci√≥n
if st.button("üîç Clasificar Comentario", type="primary"):
    if user_input.strip() != "":
        if vectorizer is not None and clf is not None:
            try:
                # Clasificaci√≥n b√°sica
                X_new = vectorizer.transform([user_input])
                prediction = clf.predict(X_new)[0]
                is_toxic = bool(prediction)
                
                # Mostrar resultado principal
                if is_toxic:
                    st.error("üö® **COMENTARIO T√ìXICO DETECTADO**")
                else:
                    st.success("‚úÖ **Comentario No T√≥xico**")
                
                # Mostrar probabilidades b√°sicas
                if show_probability:
                    prob_scores = clf.predict_proba(X_new)[0]
                    prob_toxic = prob_scores[1] if len(prob_scores) > 1 else 0.0
                    prob_safe = prob_scores[0] if len(prob_scores) > 1 else 1.0
                    
                    col_prob1, col_prob2 = st.columns(2)
                    with col_prob1:
                        st.metric("Probabilidad T√≥xico", f"{prob_toxic:.1%}")
                    with col_prob2:
                        st.metric("Probabilidad No T√≥xico", f"{prob_safe:.1%}")
                
                # An√°lisis detallado (solo si es t√≥xico y est√° disponible)
                if (show_detailed and is_toxic and detailed_available and 
                    detailed_clf is not None and detailed_vectorizer is not None and 
                    toxicity_categories is not None):
                    st.markdown("---")
                    st.subheader("üî¨ An√°lisis Detallado de Toxicidad")
                    
                    try:
                        # Clasificaci√≥n detallada
                        X_detailed = detailed_vectorizer.transform([user_input])
                        detailed_predictions = detailed_clf.predict(X_detailed)[0]
                        detailed_probabilities = detailed_clf.predict_proba(X_detailed)
                        
                        # Preparar datos para visualizaci√≥n
                        categories_data = []
                        
                        for i, category in enumerate(toxicity_categories):
                            if i < len(detailed_predictions):
                                # Obtener probabilidad
                                prob = 0.0
                                if i < len(detailed_probabilities):
                                    proba_array = detailed_probabilities[i][0]
                                    if len(proba_array) > 1:
                                        prob = float(proba_array[1])
                                
                                is_present = bool(detailed_predictions[i])
                                
                                if is_present or prob > 0.1:  # Mostrar solo relevantes
                                    categories_data.append({
                                        'Categor√≠a': category.replace('_', ' ').title(),
                                        'Detectado': '‚úÖ' if is_present else '‚ùå',
                                        'Probabilidad': f"{prob:.1%}",
                                        'Valor_Prob': prob
                                    })
                        
                        if categories_data:
                            # Ordenar por probabilidad
                            categories_data.sort(key=lambda x: x['Valor_Prob'], reverse=True)
                            
                            # Mostrar en columnas
                            col_det1, col_det2 = st.columns(2)
                            
                            # Categor√≠as detectadas
                            detected = [cat for cat in categories_data if cat['Detectado'] == '‚úÖ']
                            if detected:
                                with col_det1:
                                    st.markdown("**üéØ Categor√≠as Detectadas:**")
                                    for cat in detected:
                                        st.write(f"‚Ä¢ **{cat['Categor√≠a']}** ({cat['Probabilidad']})")
                            
                            # Categor√≠as con probabilidad alta pero no detectadas
                            suspicious = [cat for cat in categories_data if cat['Detectado'] == '‚ùå' and cat['Valor_Prob'] > 0.3]
                            if suspicious:
                                with col_det2:
                                    st.markdown("**‚ö†Ô∏è Posibles Indicadores:**")
                                    for cat in suspicious:
                                        st.write(f"‚Ä¢ {cat['Categor√≠a']} ({cat['Probabilidad']})")
                            
                            # Tabla detallada (colapsible)
                            with st.expander("üìã Ver tabla completa de an√°lisis"):
                                df_results = pd.DataFrame([
                                    {k: v for k, v in cat.items() if k != 'Valor_Prob'} 
                                    for cat in categories_data
                                ])
                                st.dataframe(df_results, use_container_width=True)
                        else:
                            st.info("No se detectaron categor√≠as espec√≠ficas de toxicidad.")
                            
                    except Exception as detail_error:
                        st.error(f"Error en an√°lisis detallado: {detail_error}")
                
                elif show_detailed and not is_toxic:
                    st.info("‚ÑπÔ∏è El an√°lisis detallado solo se realiza para comentarios clasificados como t√≥xicos.")
                
                # Timestamp
                st.caption(f"An√°lisis realizado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
            except Exception as e:
                st.error(f"Error durante la clasificaci√≥n: {e}")
        else:
            st.error("‚ùå El modelo o el vectorizador no est√°n disponibles.")
    else:
        st.warning("‚ö†Ô∏è Por favor escribe un comentario para analizar.")

# Secci√≥n de ejemplos
st.markdown("---")
with st.expander("üí° Ejemplos de comentarios para probar"):
    st.markdown("""
    **Comentarios no t√≥xicos:**
    - "Me gusta mucho este art√≠culo, muy informativo."
    - "Gracias por compartir tu opini√≥n, es muy interesante."
    
    **Comentarios potencialmente t√≥xicos:**
    - "Eres un idiota por pensar eso."
    - "Tu opini√≥n es basura y no vale nada."
    
    *Nota: Estos ejemplos son solo para fines de demostraci√≥n.*
    """)

# Footer
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: gray;'>"
    "üîé Toxic Comment Classifier | Powered by Machine Learning"
    "</div>", 
    unsafe_allow_html=True
)