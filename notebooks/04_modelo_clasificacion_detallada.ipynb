{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cbe008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Dataset shape: (90902, 45)\n",
      "CategorÃ­as disponibles: 29/29\n",
      "Filtrando comentarios tÃ³xicos...\n",
      "Comentarios tÃ³xicos iniciales: 45451\n",
      "DespuÃ©s de limpiar NaN: 12246\n",
      "obscene: 1479/12246 (12.1%)\n",
      "identity_attack: 2566/12246 (21.0%)\n",
      "insult: 10209/12246 (83.4%)\n",
      "threat: 362/12246 (3.0%)\n",
      "asian: 93/12246 (0.8%)\n",
      "atheist: 34/12246 (0.3%)\n",
      "bisexual: 6/12246 (0.0%)\n",
      "black: 977/12246 (8.0%)\n",
      "buddhist: 10/12246 (0.1%)\n",
      "christian: 536/12246 (4.4%)\n",
      "female: 1531/12246 (12.5%)\n",
      "heterosexual: 27/12246 (0.2%)\n",
      "hindu: 10/12246 (0.1%)\n",
      "homosexual_gay_or_lesbian: 583/12246 (4.8%)\n",
      "âŒ intellectual_or_learning_disability: 0/12246 - insuficientes datos\n",
      "jewish: 252/12246 (2.1%)\n",
      "latino: 50/12246 (0.4%)\n",
      "male: 1396/12246 (11.4%)\n",
      "muslim: 959/12246 (7.8%)\n",
      "âŒ other_disability: 0/12246 - insuficientes datos\n",
      "âŒ other_gender: 0/12246 - insuficientes datos\n",
      "other_race_or_ethnicity: 8/12246 (0.1%)\n",
      "other_religion: 5/12246 (0.0%)\n",
      "âŒ other_sexual_orientation: 1/12246 - insuficientes datos\n",
      "âŒ physical_disability: 0/12246 - insuficientes datos\n",
      "psychiatric_or_mental_illness: 243/12246 (2.0%)\n",
      "transgender: 91/12246 (0.7%)\n",
      "white: 1358/12246 (11.1%)\n",
      "sexual_explicit: 623/12246 (5.1%)\n",
      "\n",
      "CategorÃ­as vÃ¡lidas para entrenamiento: 24\n",
      "Datos finales: X=(12246,), y=(12246, 24)\n",
      "Train: (9796,), Test: (2450,)\n",
      "Vectorizando texto...\n",
      "Dataset shape: (90902, 45)\n",
      "CategorÃ­as disponibles: 29/29\n",
      "Filtrando comentarios tÃ³xicos...\n",
      "Comentarios tÃ³xicos iniciales: 45451\n",
      "DespuÃ©s de limpiar NaN: 12246\n",
      "obscene: 1479/12246 (12.1%)\n",
      "identity_attack: 2566/12246 (21.0%)\n",
      "insult: 10209/12246 (83.4%)\n",
      "threat: 362/12246 (3.0%)\n",
      "asian: 93/12246 (0.8%)\n",
      "atheist: 34/12246 (0.3%)\n",
      "bisexual: 6/12246 (0.0%)\n",
      "black: 977/12246 (8.0%)\n",
      "buddhist: 10/12246 (0.1%)\n",
      "christian: 536/12246 (4.4%)\n",
      "female: 1531/12246 (12.5%)\n",
      "heterosexual: 27/12246 (0.2%)\n",
      "hindu: 10/12246 (0.1%)\n",
      "homosexual_gay_or_lesbian: 583/12246 (4.8%)\n",
      "âŒ intellectual_or_learning_disability: 0/12246 - insuficientes datos\n",
      "jewish: 252/12246 (2.1%)\n",
      "latino: 50/12246 (0.4%)\n",
      "male: 1396/12246 (11.4%)\n",
      "muslim: 959/12246 (7.8%)\n",
      "âŒ other_disability: 0/12246 - insuficientes datos\n",
      "âŒ other_gender: 0/12246 - insuficientes datos\n",
      "other_race_or_ethnicity: 8/12246 (0.1%)\n",
      "other_religion: 5/12246 (0.0%)\n",
      "âŒ other_sexual_orientation: 1/12246 - insuficientes datos\n",
      "âŒ physical_disability: 0/12246 - insuficientes datos\n",
      "psychiatric_or_mental_illness: 243/12246 (2.0%)\n",
      "transgender: 91/12246 (0.7%)\n",
      "white: 1358/12246 (11.1%)\n",
      "sexual_explicit: 623/12246 (5.1%)\n",
      "\n",
      "CategorÃ­as vÃ¡lidas para entrenamiento: 24\n",
      "Datos finales: X=(12246,), y=(12246, 24)\n",
      "Train: (9796,), Test: (2450,)\n",
      "Vectorizando texto...\n",
      "Vectorized shape: (9796, 10000)\n",
      "Entrenando modelo multi-etiqueta...\n",
      "Vectorized shape: (9796, 10000)\n",
      "Entrenando modelo multi-etiqueta...\n",
      "âœ… Entrenamiento completado\n",
      "\n",
      "Evaluando modelo...\n",
      "MÃ©tricas por categorÃ­a:\n",
      "obscene: P=0.916 R=0.331 F1=0.486 Support=296\n",
      "identity_attack: P=0.870 R=0.540 F1=0.667 Support=522\n",
      "insult: P=0.872 R=0.981 F1=0.923 Support=2060\n",
      "threat: P=1.000 R=0.032 F1=0.062 Support=63\n",
      "asian: P=0.000 R=0.000 F1=0.000 Support=18\n",
      "atheist: P=0.000 R=0.000 F1=0.000 Support=7\n",
      "bisexual: P=0.000 R=0.000 F1=0.000 Support=2\n",
      "black: P=1.000 R=0.541 F1=0.703 Support=205\n",
      "buddhist: P=0.000 R=0.000 F1=0.000 Support=2\n",
      "christian: P=0.966 R=0.259 F1=0.409 Support=108\n",
      "female: P=0.989 R=0.613 F1=0.757 Support=305\n",
      "heterosexual: P=0.000 R=0.000 F1=0.000 Support=8\n",
      "hindu: P=0.000 R=0.000 F1=0.000 Support=3\n",
      "homosexual_gay_or_lesbian: P=1.000 R=0.406 F1=0.577 Support=138\n",
      "jewish: P=1.000 R=0.140 F1=0.246 Support=50\n",
      "latino: P=0.000 R=0.000 F1=0.000 Support=10\n",
      "male: P=0.896 R=0.509 F1=0.649 Support=289\n",
      "muslim: P=0.991 R=0.567 F1=0.721 Support=187\n",
      "other_race_or_ethnicity: P=0.000 R=0.000 F1=0.000 Support=1\n",
      "other_religion: P=0.000 R=0.000 F1=0.000 Support=1\n",
      "psychiatric_or_mental_illness: P=0.923 R=0.414 F1=0.571 Support=58\n",
      "transgender: P=0.000 R=0.000 F1=0.000 Support=15\n",
      "white: P=0.975 R=0.726 F1=0.832 Support=270\n",
      "sexual_explicit: P=0.895 R=0.137 F1=0.238 Support=124\n",
      "\n",
      "Guardando modelos...\n",
      "âœ… Entrenamiento completado\n",
      "\n",
      "Evaluando modelo...\n",
      "MÃ©tricas por categorÃ­a:\n",
      "obscene: P=0.916 R=0.331 F1=0.486 Support=296\n",
      "identity_attack: P=0.870 R=0.540 F1=0.667 Support=522\n",
      "insult: P=0.872 R=0.981 F1=0.923 Support=2060\n",
      "threat: P=1.000 R=0.032 F1=0.062 Support=63\n",
      "asian: P=0.000 R=0.000 F1=0.000 Support=18\n",
      "atheist: P=0.000 R=0.000 F1=0.000 Support=7\n",
      "bisexual: P=0.000 R=0.000 F1=0.000 Support=2\n",
      "black: P=1.000 R=0.541 F1=0.703 Support=205\n",
      "buddhist: P=0.000 R=0.000 F1=0.000 Support=2\n",
      "christian: P=0.966 R=0.259 F1=0.409 Support=108\n",
      "female: P=0.989 R=0.613 F1=0.757 Support=305\n",
      "heterosexual: P=0.000 R=0.000 F1=0.000 Support=8\n",
      "hindu: P=0.000 R=0.000 F1=0.000 Support=3\n",
      "homosexual_gay_or_lesbian: P=1.000 R=0.406 F1=0.577 Support=138\n",
      "jewish: P=1.000 R=0.140 F1=0.246 Support=50\n",
      "latino: P=0.000 R=0.000 F1=0.000 Support=10\n",
      "male: P=0.896 R=0.509 F1=0.649 Support=289\n",
      "muslim: P=0.991 R=0.567 F1=0.721 Support=187\n",
      "other_race_or_ethnicity: P=0.000 R=0.000 F1=0.000 Support=1\n",
      "other_religion: P=0.000 R=0.000 F1=0.000 Support=1\n",
      "psychiatric_or_mental_illness: P=0.923 R=0.414 F1=0.571 Support=58\n",
      "transgender: P=0.000 R=0.000 F1=0.000 Support=15\n",
      "white: P=0.975 R=0.726 F1=0.832 Support=270\n",
      "sexual_explicit: P=0.895 R=0.137 F1=0.238 Support=124\n",
      "\n",
      "Guardando modelos...\n",
      "âœ… Modelos guardados exitosamente!\n",
      "- detailed_model.pkl\n",
      "- vectorizer_detailed.pkl\n",
      "- toxicity_categories.pkl\n",
      "ðŸŽ‰ Proceso completado!\n",
      "âœ… Modelos guardados exitosamente!\n",
      "- detailed_model.pkl\n",
      "- vectorizer_detailed.pkl\n",
      "- toxicity_categories.pkl\n",
      "ðŸŽ‰ Proceso completado!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Cargar datos\n",
    "print(\"Cargando datos...\")\n",
    "df = pd.read_csv('../data/data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Definir las categorÃ­as de toxicidad detallada\n",
    "toxicity_categories = [\n",
    "    'obscene', 'identity_attack', 'insult', 'threat', 'asian', 'atheist', \n",
    "    'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual', \n",
    "    'hindu', 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', \n",
    "    'jewish', 'latino', 'male', 'muslim', 'other_disability', 'other_gender', \n",
    "    'other_race_or_ethnicity', 'other_religion', 'other_sexual_orientation', \n",
    "    'physical_disability', 'psychiatric_or_mental_illness', 'transgender', \n",
    "    'white', 'sexual_explicit'\n",
    "]\n",
    "\n",
    "# Verificar quÃ© categorÃ­as existen en el dataset\n",
    "available_categories = [cat for cat in toxicity_categories if cat in df.columns]\n",
    "print(f\"CategorÃ­as disponibles: {len(available_categories)}/{len(toxicity_categories)}\")\n",
    "\n",
    "# Filtrar solo comentarios tÃ³xicos (target > 0.5) con datos completos\n",
    "print(\"Filtrando comentarios tÃ³xicos...\")\n",
    "toxic_df = df[df['target'] > 0.5].copy()\n",
    "print(f\"Comentarios tÃ³xicos iniciales: {len(toxic_df)}\")\n",
    "\n",
    "# Limpiar datos - eliminar filas con valores NaN en las categorÃ­as\n",
    "for category in available_categories:\n",
    "    toxic_df = toxic_df.dropna(subset=[category])\n",
    "\n",
    "print(f\"DespuÃ©s de limpiar NaN: {len(toxic_df)}\")\n",
    "\n",
    "# Convertir a binario (umbral de 0.5)\n",
    "for category in available_categories:\n",
    "    toxic_df[category] = (toxic_df[category] > 0.5).astype(int)\n",
    "\n",
    "# Filtrar categorÃ­as que tienen al menos algunos ejemplos positivos\n",
    "valid_categories = []\n",
    "for category in available_categories:\n",
    "    positive_count = toxic_df[category].sum()\n",
    "    total_count = len(toxic_df)\n",
    "    if positive_count >= 5 and positive_count < total_count - 5:  # Al menos 5 de cada clase\n",
    "        valid_categories.append(category)\n",
    "        print(f\"{category}: {positive_count}/{total_count} ({positive_count/total_count*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"âŒ {category}: {positive_count}/{total_count} - insuficientes datos\")\n",
    "\n",
    "print(f\"\\nCategorÃ­as vÃ¡lidas para entrenamiento: {len(valid_categories)}\")\n",
    "\n",
    "if len(valid_categories) == 0:\n",
    "    raise ValueError(\"No hay categorÃ­as con suficientes datos para entrenar\")\n",
    "\n",
    "# Preparar datos solo con categorÃ­as vÃ¡lidas\n",
    "X = toxic_df['comment_text']\n",
    "y = toxic_df[valid_categories]\n",
    "\n",
    "print(f\"Datos finales: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# Dividir datos\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y.iloc[:, 0]\n",
    "    )\n",
    "except ValueError:\n",
    "    # Si falla la estratificaciÃ³n, usar divisiÃ³n simple\n",
    "    print(\"Usando divisiÃ³n aleatoria (sin estratificaciÃ³n)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Vectorizar texto\n",
    "print(\"Vectorizando texto...\")\n",
    "vectorizer_detailed = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer_detailed.fit_transform(X_train)\n",
    "X_test_vec = vectorizer_detailed.transform(X_test)\n",
    "print(f\"Vectorized shape: {X_train_vec.shape}\")\n",
    "\n",
    "# Entrenar modelo multi-etiqueta\n",
    "print(\"Entrenando modelo multi-etiqueta...\")\n",
    "detailed_classifier = MultiOutputClassifier(\n",
    "    LogisticRegression(random_state=42, max_iter=1000)\n",
    ")\n",
    "\n",
    "detailed_classifier.fit(X_train_vec, y_train)\n",
    "print(\"âœ… Entrenamiento completado\")\n",
    "\n",
    "# EvaluaciÃ³n simplificada\n",
    "print(\"\\nEvaluando modelo...\")\n",
    "y_pred = detailed_classifier.predict(X_test_vec)\n",
    "\n",
    "print(\"MÃ©tricas por categorÃ­a:\")\n",
    "for i, category in enumerate(valid_categories):\n",
    "    y_true = y_test.iloc[:, i]\n",
    "    y_pred_cat = y_pred[:, i]\n",
    "    \n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred_cat, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_cat, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_cat, zero_division=0)\n",
    "    support = y_true.sum()\n",
    "    \n",
    "    print(f\"{category}: P={precision:.3f} R={recall:.3f} F1={f1:.3f} Support={support}\")\n",
    "\n",
    "# Crear directorio artifacts_detail si no existe\n",
    "os.makedirs('../artifacts_detail', exist_ok=True)\n",
    "\n",
    "# Guardar modelos\n",
    "print(\"\\nGuardando modelos...\")\n",
    "joblib.dump(detailed_classifier, '../artifacts_detail/detailed_model.pkl')\n",
    "joblib.dump(vectorizer_detailed, '../artifacts_detail/vectorizer_detailed.pkl')\n",
    "joblib.dump(valid_categories, '../artifacts_detail/toxicity_categories.pkl')\n",
    "\n",
    "print(\"âœ… Modelos guardados exitosamente!\")\n",
    "print(f\"- detailed_model.pkl\")\n",
    "print(f\"- vectorizer_detailed.pkl\")\n",
    "print(f\"- toxicity_categories.pkl\")\n",
    "\n",
    "print(\"ðŸŽ‰ Proceso completado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
